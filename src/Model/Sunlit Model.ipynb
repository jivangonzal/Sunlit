{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1a735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('/media/Data/HexTokenizer')\n",
    "tokens = tokenizer('88a20 8a204 a2043 20439')\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3808b254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datasets import *\n",
    "\n",
    "paths = [str(x) for x in Path('/media/Data/onlytext').glob('**/*.csv')]\n",
    "\n",
    "dataset = load_dataset(\"text\", cache_dir='/media/Data/images', data_files=paths, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3856c4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(examples):\n",
    "  \n",
    "  return tokenizer(examples[\"text\"], return_special_tokens_mask=True)\n",
    "\n",
    "train_dataset = dataset.map(encode, batched=True, batch_size = 20000, num_proc = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940bd52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datasets import *\n",
    "\n",
    "paths = [str(Path('/media/Data/onlytexttest/2019-03-08-13-24-30-192.168.1.197-8.final.csv'))]\n",
    "\n",
    "train_dataset = load_dataset(\"text\", cache_dir='/media/Data/images', data_files=paths, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535aa8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(examples):\n",
    "  \"\"\"Mapping function to tokenize the sentences passed without truncation\"\"\"\n",
    "  return tokenizer(examples[\"text\"], return_special_tokens_mask=True)\n",
    "\n",
    "test_dataset = dataset.map(encode, batched=True, batch_size = 20000, num_proc = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fa0ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
    "test_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a1a3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum sequence length, lowering will result to faster training (when increasing batch size)\n",
    "max_length = 512\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= max_length:\n",
    "        total_length = (total_length // max_length) * max_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + max_length] for i in range(0, total_length, max_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c1a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "max_length = 512\n",
    "train_dataset = train_dataset.map(group_texts, batched=True,\n",
    "                                    desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "test_dataset = test_dataset.map(group_texts, batched=True,\n",
    "                                    desc=f\"Grouping texts in chunks of {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bc6acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(\"torch\")\n",
    "test_dataset.set_format(\"torch\")\n",
    "\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d295eadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f05018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6ff1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the data collator, randomly masking 20% (default is 15%) of the tokens for the Masked Language\n",
    "# Modeling (MLM) task\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb3b32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/media/Data/pretrained-bert\"\n",
    "# make the directory if not already there\n",
    "if not os.path.isdir(model_path):\n",
    "  os.mkdir(model_path)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,          # output directory to where save model checkpoint\n",
    "    evaluation_strategy=\"steps\",    # evaluate each `logging_steps` steps\n",
    "    overwrite_output_dir=True,      \n",
    "    num_train_epochs=10,            # number of training epochs, feel free to tweak\n",
    "    per_device_train_batch_size=6, # the training batch size, put it as high as your GPU memory fits\n",
    "    gradient_accumulation_steps=8,  # accumulating the gradients before updating the weights\n",
    "    per_device_eval_batch_size=32,  # evaluation batch size\n",
    "    logging_steps=1000,             # evaluate, log and save model checkpoints every 1000 step\n",
    "    save_steps=1000,\n",
    "    # load_best_model_at_end=True,  # whether to load the best model (in terms of loss) at the end of training\n",
    "    # save_total_limit=3,           # whether you don't have much space so you let only 3 model weights saved in the disk\n",
    ")\n",
    "\n",
    "# initialize the trainer and pass everything to it\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# train the model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
