{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b1db28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 141, 146, 150, 157, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('/media/Data/HexTokenizer')\n",
    "tokens = tokenizer('88a20 8a204 a2043 20439')\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8445eb1b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c142b1e1bd4c63f4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /media/Data/images/text/default-c142b1e1bd4c63f4/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5a05f4cd034f9f9fc6b5aaa0bd8c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e605508846482b93cdd272d9b1f4e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /media/Data/images/text/default-c142b1e1bd4c63f4/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datasets import *\n",
    "\n",
    "train_paths = [str(x) for x in Path('/media/Data/onlytext').glob('**/*.csv')]\n",
    "dataset = load_dataset(\"text\", cache_dir='/media/Data/images', data_files=train_paths, split=\"train\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "435a3952",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text'],\n",
       "     num_rows: 6049254\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text'],\n",
       "     num_rows: 61104\n",
       " }))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = dataset.train_test_split(test_size=0.01)\n",
    "\n",
    "d[\"train\"], d[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cc6b7c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247af8b903e04867af5b3ce72ee06c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6050 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def encode(examples):\n",
    "  \n",
    "  return tokenizer(examples[\"text\"], return_special_tokens_mask=True)\n",
    "\n",
    "train_dataset = d[\"train\"].map(encode, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4ab3f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "    num_rows: 6049254\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9727b35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.save_to_disk(\"/media/Data/tmp/train.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a4763c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a22fda30ee24957a2926e129d4ed2f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = d[\"test\"].map(encode, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2734cf34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "    num_rows: 61104\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23c8efe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.save_to_disk(\"/media/Data/tmp/test.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cc81bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "\n",
    "train_dataset = load_from_disk(\"/media/Data/tmp/train.hf\")\n",
    "test_dataset = load_from_disk(\"/media/Data/tmp/test.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c569346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum sequence length, lowering will result to faster training (when increasing batch size)\n",
    "max_length = 512\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= max_length:\n",
    "        total_length = (total_length // max_length) * max_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + max_length] for i in range(0, total_length, max_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37758a29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0a7cb990294e4f87839a29b54a754c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 512:   0%|          | 0/6050 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "train_dataset = train_dataset.map(group_texts, batched=True,\n",
    "                                    desc=f\"Grouping texts in chunks of {max_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ff3c892",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.save_to_disk(\"/media/Data/tmp/trainnew.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9c6bc9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2450fc16f944b6bb9b81c20d143110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 512:   0%|          | 0/62 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = test_dataset.map(group_texts, batched=True,\n",
    "                                    desc=f\"Grouping texts in chunks of {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "644e3206",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dataset.save_to_disk(\"/media/Data/tmp/testnew.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0ea6388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "\n",
    "train_dataset = load_from_disk(\"/media/Data/tmp/trainnew.hf\")\n",
    "test_dataset = load_from_disk(\"/media/Data/tmp/testnew.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a74a599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2029411, 20480)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.set_format(\"torch\")\n",
    "test_dataset.set_format(\"torch\")\n",
    "\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41fd4d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "595d1354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import *\n",
    "\n",
    "max_length = 512\n",
    "# 30,522 vocab is BERT's default vocab size, feel free to tweak\n",
    "vocab_size = 30_522\n",
    "\n",
    "\n",
    "model_config = BertConfig(vocab_size=vocab_size, max_position_embeddings=max_length)\n",
    "model = BertForMaskedLM(config=model_config)\n",
    "#model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af13421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the data collator, randomly masking 20% (default is 15%) of the tokens for the Masked Language\n",
    "# Modeling (MLM) task\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c707a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running training *****\n",
      "  Num examples = 2029411\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 6\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 422790\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27001' max='422790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 27001/422790 25:35:31 < 375:09:52, 0.29 it/s, Epoch 0.64/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>6.844700</td>\n",
       "      <td>6.572571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>6.531400</td>\n",
       "      <td>6.442201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>6.243400</td>\n",
       "      <td>5.386679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>5.072400</td>\n",
       "      <td>4.488007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.303500</td>\n",
       "      <td>2.295897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.950600</td>\n",
       "      <td>1.385920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.198500</td>\n",
       "      <td>0.777766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.705100</td>\n",
       "      <td>0.443978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.425900</td>\n",
       "      <td>0.254057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.260900</td>\n",
       "      <td>0.158311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.173600</td>\n",
       "      <td>0.101698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.121400</td>\n",
       "      <td>0.067719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.087900</td>\n",
       "      <td>0.046694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.068000</td>\n",
       "      <td>0.036004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.055400</td>\n",
       "      <td>0.030356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.048100</td>\n",
       "      <td>0.025553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.043000</td>\n",
       "      <td>0.023757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.038800</td>\n",
       "      <td>0.022634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.036100</td>\n",
       "      <td>0.020024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.019442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.032100</td>\n",
       "      <td>0.018794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.018151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.029200</td>\n",
       "      <td>0.017049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>0.016991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>0.016337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>0.015583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='248' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [248/640 04:21 < 06:55, 0.94 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20480\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /media/Data/pretrained-bert/checkpoint-1000\n",
      "Configuration saved in /media/Data/pretrained-bert/checkpoint-1000/config.json\n",
      "Model weights saved in /media/Data/pretrained-bert/checkpoint-1000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20480\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /media/Data/pretrained-bert/checkpoint-2000\n",
      "Configuration saved in /media/Data/pretrained-bert/checkpoint-2000/config.json\n",
      "Model weights saved in /media/Data/pretrained-bert/checkpoint-2000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20480\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /media/Data/pretrained-bert/checkpoint-3000\n",
      "Configuration saved in /media/Data/pretrained-bert/checkpoint-3000/config.json\n",
      "Model weights saved in /media/Data/pretrained-bert/checkpoint-3000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20480\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /media/Data/pretrained-bert/checkpoint-4000\n",
      "Configuration saved in /media/Data/pretrained-bert/checkpoint-4000/config.json\n",
      "Model weights saved in /media/Data/pretrained-bert/checkpoint-4000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20480\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /media/Data/pretrained-bert/checkpoint-5000\n",
      "Configuration saved in /media/Data/pretrained-bert/checkpoint-5000/config.json\n",
      "Model weights saved in /media/Data/pretrained-bert/checkpoint-5000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20480\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /media/Data/pretrained-bert/checkpoint-6000\n",
      "Configuration saved in /media/Data/pretrained-bert/checkpoint-6000/config.json\n",
      "Model weights saved in /media/Data/pretrained-bert/checkpoint-6000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20480\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /media/Data/pretrained-bert/checkpoint-7000\n",
      "Configuration saved in /media/Data/pretrained-bert/checkpoint-7000/config.json\n",
      "Model weights saved in /media/Data/pretrained-bert/checkpoint-7000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20480\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /media/Data/pretrained-bert/checkpoint-8000\n",
      "Configuration saved in /media/Data/pretrained-bert/checkpoint-8000/config.json\n",
      "Model weights saved in /media/Data/pretrained-bert/checkpoint-8000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20480\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /media/Data/pretrained-bert/checkpoint-9000\n",
      "Configuration saved in /media/Data/pretrained-bert/checkpoint-9000/config.json\n",
      "Model weights saved in /media/Data/pretrained-bert/checkpoint-9000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20480\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /media/Data/pretrained-bert/checkpoint-10000\n",
      "Configuration saved in /media/Data/pretrained-bert/checkpoint-10000/config.json\n",
      "Model weights saved in /media/Data/pretrained-bert/checkpoint-10000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20480\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /media/Data/pretrained-bert/checkpoint-11000\n",
      "Configuration saved in /media/Data/pretrained-bert/checkpoint-11000/config.json\n",
      "Model weights saved in /media/Data/pretrained-bert/checkpoint-11000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20480\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /media/Data/pretrained-bert/checkpoint-12000\n",
      "Configuration saved in /media/Data/pretrained-bert/checkpoint-12000/config.json\n",
      "Model weights saved in /media/Data/pretrained-bert/checkpoint-12000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20480\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /media/Data/pretrained-bert/checkpoint-13000\n",
      "Configuration saved in /media/Data/pretrained-bert/checkpoint-13000/config.json\n",
      "Model weights saved in /media/Data/pretrained-bert/checkpoint-13000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20480\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /media/Data/pretrained-bert/checkpoint-14000\n",
      "Configuration saved in /media/Data/pretrained-bert/checkpoint-14000/config.json\n",
      "Model weights saved in /media/Data/pretrained-bert/checkpoint-14000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20480\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /media/Data/pretrained-bert/checkpoint-15000\n",
      "Configuration saved in /media/Data/pretrained-bert/checkpoint-15000/config.json\n",
      "Model weights saved in /media/Data/pretrained-bert/checkpoint-15000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20480\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /media/Data/pretrained-bert/checkpoint-16000\n",
      "Configuration saved in /media/Data/pretrained-bert/checkpoint-16000/config.json\n",
      "Model weights saved in /media/Data/pretrained-bert/checkpoint-16000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20480\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /media/Data/pretrained-bert/checkpoint-17000\n",
      "Configuration saved in /media/Data/pretrained-bert/checkpoint-17000/config.json\n",
      "Model weights saved in /media/Data/pretrained-bert/checkpoint-17000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20480\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /media/Data/pretrained-bert/checkpoint-18000\n",
      "Configuration saved in /media/Data/pretrained-bert/checkpoint-18000/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in /media/Data/pretrained-bert/checkpoint-18000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20480\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /media/Data/pretrained-bert/checkpoint-19000\n",
      "Configuration saved in /media/Data/pretrained-bert/checkpoint-19000/config.json\n",
      "Model weights saved in /media/Data/pretrained-bert/checkpoint-19000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20480\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /media/Data/pretrained-bert/checkpoint-20000\n",
      "Configuration saved in /media/Data/pretrained-bert/checkpoint-20000/config.json\n",
      "Model weights saved in /media/Data/pretrained-bert/checkpoint-20000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20480\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /media/Data/pretrained-bert/checkpoint-21000\n",
      "Configuration saved in /media/Data/pretrained-bert/checkpoint-21000/config.json\n",
      "Model weights saved in /media/Data/pretrained-bert/checkpoint-21000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20480\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /media/Data/pretrained-bert/checkpoint-22000\n",
      "Configuration saved in /media/Data/pretrained-bert/checkpoint-22000/config.json\n",
      "Model weights saved in /media/Data/pretrained-bert/checkpoint-22000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20480\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /media/Data/pretrained-bert/checkpoint-23000\n",
      "Configuration saved in /media/Data/pretrained-bert/checkpoint-23000/config.json\n",
      "Model weights saved in /media/Data/pretrained-bert/checkpoint-23000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20480\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /media/Data/pretrained-bert/checkpoint-24000\n",
      "Configuration saved in /media/Data/pretrained-bert/checkpoint-24000/config.json\n",
      "Model weights saved in /media/Data/pretrained-bert/checkpoint-24000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20480\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /media/Data/pretrained-bert/checkpoint-25000\n",
      "Configuration saved in /media/Data/pretrained-bert/checkpoint-25000/config.json\n",
      "Model weights saved in /media/Data/pretrained-bert/checkpoint-25000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20480\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /media/Data/pretrained-bert/checkpoint-26000\n",
      "Configuration saved in /media/Data/pretrained-bert/checkpoint-26000/config.json\n",
      "Model weights saved in /media/Data/pretrained-bert/checkpoint-26000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20480\n",
      "  Batch size = 32\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "model_path = \"/media/Data/pretrained-bert\"\n",
    "# make the directory if not already there\n",
    "if not os.path.isdir(model_path):\n",
    "  os.mkdir(model_path)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,          # output directory to where save model checkpoint\n",
    "    evaluation_strategy=\"steps\",    # evaluate each `logging_steps` steps\n",
    "    overwrite_output_dir=True,      \n",
    "    num_train_epochs=10,            # number of training epochs, feel free to tweak\n",
    "    per_device_train_batch_size=6, # the training batch size, put it as high as your GPU memory fits\n",
    "    gradient_accumulation_steps=8,  # accumulating the gradients before updating the weights\n",
    "    per_device_eval_batch_size=32,  # evaluation batch size\n",
    "    logging_steps=1000,             # evaluate, log and save model checkpoints every 1000 step\n",
    "    save_steps=1000,\n",
    "    # load_best_model_at_end=True,  # whether to load the best model (in terms of loss) at the end of training\n",
    "    # save_total_limit=3,           # whether you don't have much space so you let only 3 model weights saved in the disk\n",
    ")\n",
    "\n",
    "# initialize the trainer and pass everything to it\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bf1917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
